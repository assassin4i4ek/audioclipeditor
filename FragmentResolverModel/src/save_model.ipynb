{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from dataset import *\n",
    "from model import *\n",
    "import time\n",
    "import generated.fragment_resolver_pb2 as fragment_resolver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "sample_rate = 32758\n",
    "min_duration_sec = 5\n",
    "num_grid_cells = 20\n",
    "confidence = 0.5\n",
    "min_overlap_sec = min_duration_sec / num_grid_cells * 4\n",
    "yolo_model_weights_path = os.path.join(cwd, '..', 'results', 'trained_models', '7_2_cpt.h5')\n",
    "model_name = 'model7_2'\n",
    "input_length = int(sample_rate * min_duration_sec)\n",
    "\n",
    "model_params = {\n",
    "    fragment_resolver.ResolvedTransformer.Type.SILENCE: {'silenceDurationUs': LinearTransformerNormalizerLayer(5e5, in_dtype=tf.int64)},\n",
    "#     fragment_resolver.ResolvedTransformer.Type.TYPE2: {\n",
    "#               'typeType2Param2': LinearTransformerNormalizerLayer(1, in_dtype=tf.int64), \n",
    "#               'typeType2Param3': LinearTransformerNormalizerLayer(2, in_dtype=tf.int64), \n",
    "#               'typeType2Param4': LinearTransformerNormalizerLayer(10, in_dtype=tf.int64)\n",
    "#              },\n",
    "#     fragment_resolver.ResolvedTransformer.Type.TYPE3: {\n",
    "#               'typeType3Param5': LinearTransformerNormalizerLayer(5, in_dtype=tf.int64), \n",
    "#               'typeType3Param6': LinearTransformerNormalizerLayer(5, in_dtype=tf.int64)\n",
    "#              }\n",
    "}\n",
    "\n",
    "config = fragment_resolver.FragmentResolverModelConfig()\n",
    "config.sampleRate = sample_rate\n",
    "encoding_type = 'CENTER_DURATION'\n",
    "fragments_dtype = tf.float32.name\n",
    "\n",
    "with open(os.path.join(cwd, 'generated', 'descriptor_set.desc'), 'rb') as desc_file:\n",
    "    protobuf_descriptor = desc_file.read()\n",
    "\n",
    "fragment_encoder = ProtoFragmentBatchEncoderLayer(sample_rate, model_params, protobuf_descriptor, fragments_dtype)    \n",
    "audio_requests = tf.keras.layers.Input(1, dtype=tf.string, name='fragment_resolver_model_requests')\n",
    "ragged_samples = AudioProcessRequestDecoderLayer(protobuf_descriptor, name='ragged_samples')(audio_requests)\n",
    "padded_samples = AudioDataPadderLayer(sample_rate, min_duration_sec, min_duration_sec * 0.01, name='padded_samples')(ragged_samples)\n",
    "frames_of_samples, frame_offsets = AudioDataUniformSplitterLayer(sample_rate, min_duration_sec, min_overlap_sec, min_duration_sec * 0.01, 'SAMPLE', name='splitted_samples')(padded_samples)\n",
    "predicted_yolo_output_frames_batch = YoloLayer(input_length, num_grid_cells, fragment_encoder.transformer_output_length, yolo_model_weights_path, name='yolo_model')(frames_of_samples)\n",
    "decoded_fragments = YoloOutputBatchDecoderLayer(input_length, confidence, encoding_type, fragments_dtype, name='yolo_output_decoder')(predicted_yolo_output_frames_batch, frame_offsets)\n",
    "resolved_fragments = FragmentBatchResolverLayer(sample_rate, min_duration_sec, num_grid_cells, name='fragment_resolver')(decoded_fragments, frame_offsets)\n",
    "encoded_fragment_protos = ProtoFragmentBatchDecoderLayer(sample_rate, model_params, protobuf_descriptor, name='audio_proto_encoder')(resolved_fragments)\n",
    "resolved_fragments = tf.keras.layers.Lambda(lambda x: x, name='resolved_fragments_responses')(encoded_fragment_protos)\n",
    "\n",
    "model = FragmentResolverModel(audio_requests, resolved_fragments, config, name='my_model')\n",
    "\n",
    "audio_decoder = AudioDecoder(sample_rate)\n",
    "\n",
    "# test_filepath1 = os.path.join(cwd, '..', 'data', 'clips', 'normalized', 'test1.mp3')\n",
    "# test_filepath2 = os.path.join(cwd, '..', 'data', 'clips', 'normalized', 'test2.mp3')\n",
    "\n",
    "# a1 = audio_decoder.decode(test_filepath1)\n",
    "# a2 = audio_decoder.decode(test_filepath2)\n",
    "\n",
    "# in1 = fragment_resolver.FragmentResolverModelRequest()\n",
    "# in1.audioSamplesChannel1 = a1.numpy().tobytes()\n",
    "# a1 = tf.constant(in1.SerializeToString())\n",
    "\n",
    "# in2 = fragment_resolver.FragmentResolverModelRequest()\n",
    "# in2.audioSamplesChannel1 = a2.numpy().tobytes()\n",
    "# a2 = tf.constant(in2.SerializeToString())\n",
    "\n",
    "# print(model.resolve(tf.reshape(a1, [-1, 1])))\n",
    "# print(model.resolve(tf.reshape(tf.stack([a1, a2, a1]), [-1, 1])))\n",
    "# print(model.config())\n",
    "\n",
    "display(model.summary())\n",
    "\n",
    "save_path = os.path.join(cwd, '..', 'results', 'saved_models', model_name) \n",
    "signatures = {\n",
    "    'resolve': model.resolve,\n",
    "    'config': model.config\n",
    "}\n",
    "tf.keras.models.save_model(model, save_path, include_optimizer=False, save_traces=True, signatures=signatures)\n",
    "\n",
    "[l.output_shape for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(\n",
    "    save_path, compile=False, \n",
    "    custom_objects={\n",
    "        'AudioProcessRequestDecoderLayer': AudioProcessRequestDecoderLayer,\n",
    "        'AudioDataPadderLayer': AudioDataPadderLayer,\n",
    "        'AudioDataUniformSplitterLayer': AudioDataUniformSplitterLayer,\n",
    "        'YoloLayer': YoloLayer,\n",
    "        'YoloOutputBatchDecoderLayer': YoloOutputBatchDecoderLayer,\n",
    "        'FragmentBatchResolverLayer': FragmentBatchResolverLayer,\n",
    "        'ProtoFragmentBatchDecoderLayer': ProtoFragmentBatchDecoderLayer,\n",
    "        'LinearTransformerNormalizerLayer': LinearTransformerNormalizerLayer\n",
    "    }\n",
    ")\n",
    "# print(new_model(tf.stack([a1, a2, a1, a2])))\n",
    "config = fragment_resolver.FragmentResolverModelConfig()\n",
    "config.ParseFromString(new_model.config()['config'].numpy())\n",
    "print(config)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca4e8c235f10f497534f8ad40408ba8180408f4f803f54ea5904b45c22e53726"
  },
  "kernelspec": {
   "display_name": "audio-clips-editor",
   "language": "python",
   "name": "audio-clips-editor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
