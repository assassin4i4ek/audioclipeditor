{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from dataset import *\n",
    "from model import *\n",
    "import time\n",
    "import generated.fragment_resolver_pb2 as fragment_resolver\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n",
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n",
      "{'fragment_resolver_model_responses': <tf.Tensor: shape=(1, 1), dtype=string, numpy=\n",
      "array([[b'\\n\\x17\\x08\\xb6\\xf7\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x10\\x9c\\x88.\\x1a\\x06\\x08\\x00\\x10\\xbf\\xdc\\x11\\n\\x11\\x08\\xe5\\xca}\\x10\\x8a\\xe9\\x8e\\x01\\x1a\\x06\\x08\\x00\\x10\\xde\\xf6\\x1a\\n\\x12\\x08\\xad\\x8a\\xb3\\x03\\x10\\xc7\\xa4\\xc7\\x03\\x1a\\x06\\x08\\x00\\x10\\x92\\xb7\\x1a\\n\\x12\\x08\\xbb\\xde\\xf1\\x04\\x10\\xfb\\xfb\\x81\\x05\\x1a\\x06\\x08\\x00\\x10\\x83\\xfd\\x19\\n\\x12\\x08\\xc8\\xff\\x9b\\x06\\x10\\xc0\\xce\\xad\\x06\\x1a\\x06\\x08\\x00\\x10\\x94\\xbb\\x1a\\n\\x12\\x08\\x8e\\xba\\xff\\x07\\x10\\xbe\\xad\\x9e\\x08\\x1a\\x06\\x08\\x00\\x10\\x9b\\xb3\\x1a\\n\\x12\\x08\\x8c\\xa7\\x85\\t\\x10\\xac\\x86\\xbe\\t\\x1a\\x06\\x08\\x00\\x10\\xa7\\xd9\\x11']],\n",
      "      dtype=object)>}\n",
      "{'fragment_resolver_model_responses': <tf.Tensor: shape=(3, 1), dtype=string, numpy=\n",
      "array([[b'\\n\\x17\\x08\\xb6\\xf7\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x10\\x9c\\x88.\\x1a\\x06\\x08\\x00\\x10\\xbf\\xdc\\x11\\n\\x11\\x08\\xe5\\xca}\\x10\\x8a\\xe9\\x8e\\x01\\x1a\\x06\\x08\\x00\\x10\\xde\\xf6\\x1a\\n\\x12\\x08\\xad\\x8a\\xb3\\x03\\x10\\xc7\\xa4\\xc7\\x03\\x1a\\x06\\x08\\x00\\x10\\x92\\xb7\\x1a\\n\\x12\\x08\\xbb\\xde\\xf1\\x04\\x10\\xfb\\xfb\\x81\\x05\\x1a\\x06\\x08\\x00\\x10\\x83\\xfd\\x19\\n\\x12\\x08\\xc8\\xff\\x9b\\x06\\x10\\xc0\\xce\\xad\\x06\\x1a\\x06\\x08\\x00\\x10\\x94\\xbb\\x1a\\n\\x12\\x08\\x8e\\xba\\xff\\x07\\x10\\xbe\\xad\\x9e\\x08\\x1a\\x06\\x08\\x00\\x10\\x9b\\xb3\\x1a\\n\\x12\\x08\\x8c\\xa7\\x85\\t\\x10\\xac\\x86\\xbe\\t\\x1a\\x06\\x08\\x00\\x10\\xa7\\xd9\\x11'],\n",
      "       [b\"\\n\\x0f\\x08\\xac'\\x10\\xd6\\xc3\\x06\\x1a\\x06\\x08\\x00\\x10\\xf0\\x97\\x14\\n\\x12\\x08\\xd4\\xec\\xfd\\x02\\x10\\x86\\xd2\\xa4\\x03\\x1a\\x06\\x08\\x00\\x10\\xbf\\x80\\x15\"],\n",
      "       [b'\\n\\x17\\x08\\xb6\\xf7\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x10\\x9c\\x88.\\x1a\\x06\\x08\\x00\\x10\\xbf\\xdc\\x11\\n\\x11\\x08\\xe5\\xca}\\x10\\x8a\\xe9\\x8e\\x01\\x1a\\x06\\x08\\x00\\x10\\xde\\xf6\\x1a\\n\\x12\\x08\\xad\\x8a\\xb3\\x03\\x10\\xc7\\xa4\\xc7\\x03\\x1a\\x06\\x08\\x00\\x10\\x92\\xb7\\x1a\\n\\x12\\x08\\xbb\\xde\\xf1\\x04\\x10\\xfb\\xfb\\x81\\x05\\x1a\\x06\\x08\\x00\\x10\\x83\\xfd\\x19\\n\\x12\\x08\\xc8\\xff\\x9b\\x06\\x10\\xc0\\xce\\xad\\x06\\x1a\\x06\\x08\\x00\\x10\\x94\\xbb\\x1a\\n\\x12\\x08\\x8e\\xba\\xff\\x07\\x10\\xbe\\xad\\x9e\\x08\\x1a\\x06\\x08\\x00\\x10\\x9b\\xb3\\x1a\\n\\x12\\x08\\x8c\\xa7\\x85\\t\\x10\\xac\\x86\\xbe\\t\\x1a\\x06\\x08\\x00\\x10\\xa7\\xd9\\x11']],\n",
      "      dtype=object)>}\n",
      "{'config': <tf.Tensor: shape=(), dtype=string, numpy=b'\\x08\\xf6\\xff\\x01'>}\n",
      "Model: \"my_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "fragment_resolver_model_request [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ragged_samples (AudioProcessReq (None, None, 1)      0           fragment_resolver_model_requests[\n",
      "__________________________________________________________________________________________________\n",
      "padded_samples (AudioDataPadder (None, None, 1)      0           ragged_samples[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "splitted_samples (AudioDataUnif ((None, None, None,  0           padded_samples[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "yolo_model (YoloLayer)          (None, None, 20, 5)  1597175     splitted_samples[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_decoder (YoloOutput (None, None, None, 4 0           yolo_model[0][0]                 \n",
      "                                                                 splitted_samples[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "fragment_resolver (FragmentBatc (None, None, 4)      0           yolo_output_decoder[0][0]        \n",
      "                                                                 splitted_samples[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "audio_proto_encoder (ProtoFragm (None, 1)            0           fragment_resolver[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "resolved_fragments_responses (L (None, 1)            0           audio_proto_encoder[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,597,175\n",
      "Trainable params: 1,597,175\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <dataset.proto_codec.normalizers.LinearTransformerNormalizerLayer object at 0x0000025FCD7406D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_fragments_after_padding, _get_adjusted_fragments_in_bounds, _split_fragments_into_frames, _split_into_frames_map_fn_alias, normalize while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Admin\\MyProjects\\AudioClipsEditor\\AudioClipsEditorApp\\FragmentResolverModel\\src\\..\\results\\saved_models\\model7_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Admin\\MyProjects\\AudioClipsEditor\\AudioClipsEditorApp\\FragmentResolverModel\\src\\..\\results\\saved_models\\model7_2\\assets\n",
      "C:\\Users\\Admin\\MyProjects\\AudioClipsEditor\\AudioClipsEditorApp\\.gradle\\python\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(None, 1)],\n",
       " (None, None, 1),\n",
       " (None, None, 1),\n",
       " ((None, None, None, 1), (None, None)),\n",
       " (None, None, 20, 5),\n",
       " (None, None, None, 4),\n",
       " (None, None, 4),\n",
       " (None, 1),\n",
       " (None, 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "sample_rate = 32758\n",
    "min_duration_sec = 5\n",
    "num_grid_cells = 20\n",
    "confidence = 0.5\n",
    "min_overlap_sec = min_duration_sec / num_grid_cells * 4\n",
    "yolo_model_weights_path = os.path.join(cwd, '..', 'results', 'trained_models', '7_2.h5')\n",
    "model_name = 'model7_2'\n",
    "input_length = int(sample_rate * min_duration_sec)\n",
    "\n",
    "model_params = {\n",
    "    fragment_resolver.ResolvedTransformer.Type.SILENCE: {'silenceDurationUs': LinearTransformerNormalizerLayer(5e5, in_dtype=tf.int64)},\n",
    "#     audio_process_pipeline.Transformer.TransformerType.TYPE2: {'typeType2Param2': LinearTransformerNormalizerLayer(1, in_dtype=tf.int64), \n",
    "#               'typeType2Param3': LinearTransformerNormalizerLayer(2, in_dtype=tf.int64), \n",
    "#               'typeType2Param4': LinearTransformerNormalizerLayer(10, in_dtype=tf.int64)\n",
    "#              },\n",
    "#     audio_process_pipeline.Transformer.TransformerType.TYPE3: {\n",
    "#               'typeType3Param5': LinearTransformerNormalizerLayer(5, in_dtype=tf.int64), \n",
    "#               'typeType3Param6': LinearTransformerNormalizerLayer(5, in_dtype=tf.int64)\n",
    "#              }\n",
    "}\n",
    "\n",
    "config = fragment_resolver.FragmentResolverModelConfig()\n",
    "config.sampleRate = sample_rate\n",
    "encoding_type = 'CENTER_DURATION'\n",
    "fragments_dtype = tf.float32.name\n",
    "\n",
    "with open(os.path.join(cwd, 'generated', 'descriptor_set.desc'), 'rb') as desc_file:\n",
    "    protobuf_descriptor = desc_file.read()\n",
    "\n",
    "fragment_encoder = ProtoFragmentBatchEncoderLayer(sample_rate, model_params, protobuf_descriptor, fragments_dtype)    \n",
    "audio_requests = tf.keras.layers.Input(1, dtype=tf.string, name='fragment_resolver_model_requests')\n",
    "ragged_samples = AudioProcessRequestDecoderLayer(protobuf_descriptor, name='ragged_samples')(audio_requests)\n",
    "\n",
    "### ragged_samples = AudioDataShiftAugmenterLayer(sample_rate, 4)(ragged_samples)\n",
    "\n",
    "padded_samples = AudioDataPadderLayer(sample_rate, min_duration_sec, min_duration_sec * 0.01, name='padded_samples')(ragged_samples)\n",
    "### frames_of_samples = AudioDataStepwiseSplitterLayer(sample_rate, min_duration_sec, min_duration_sec - min_overlap_sec)(padded_samples)\n",
    "frames_of_samples, frame_offsets = AudioDataUniformSplitterLayer(sample_rate, min_duration_sec, min_overlap_sec, min_duration_sec * 0.01, 'SAMPLE', name='splitted_samples')(padded_samples)\n",
    "predicted_yolo_output_frames_batch = YoloLayer(input_length, num_grid_cells, fragment_encoder.transformer_output_length, yolo_model_weights_path, name='yolo_model')(frames_of_samples)\n",
    "decoded_fragments = YoloOutputBatchDecoderLayer(input_length, confidence, encoding_type, fragments_dtype, name='yolo_output_decoder')(predicted_yolo_output_frames_batch, frame_offsets)\n",
    "resolved_fragments = FragmentBatchResolverLayer(sample_rate, min_duration_sec, num_grid_cells, name='fragment_resolver')(decoded_fragments, frame_offsets)\n",
    "encoded_fragment_protos = ProtoFragmentBatchDecoderLayer(sample_rate, model_params, protobuf_descriptor, name='audio_proto_encoder')(resolved_fragments)\n",
    "resolved_fragments = tf.keras.layers.Lambda(lambda x: x, name='resolved_fragments_responses')(encoded_fragment_protos)\n",
    "\n",
    "model = FragmentResolverModel(audio_requests, resolved_fragments, config, name='my_model')\n",
    "\n",
    "audio_decoder = AudioDecoder(sample_rate)\n",
    "\n",
    "test_filepath1 = os.path.join(cwd, '..', 'data', 'clips', 'normalized', 'ТерміналАкція20.08.mp3')\n",
    "test_filepath2 = os.path.join(cwd, '..', 'data', 'clips', 'normalized', 'ТерміналКіно9.07.mp3')\n",
    "\n",
    "a1 = audio_decoder.decode(test_filepath1)\n",
    "a2 = audio_decoder.decode(test_filepath2)\n",
    "\n",
    "in1 = fragment_resolver.FragmentResolverModelRequest()\n",
    "in1.audioSamplesChannel1 = a1.numpy().tobytes()\n",
    "a1 = tf.constant(in1.SerializeToString())\n",
    "\n",
    "in2 = fragment_resolver.FragmentResolverModelRequest()\n",
    "in2.audioSamplesChannel1 = a2.numpy().tobytes()\n",
    "a2 = tf.constant(in2.SerializeToString())\n",
    "\n",
    "print(model.resolve(tf.reshape(a1, [-1, 1])))\n",
    "print(model.resolve(tf.reshape(tf.stack([a1, a2, a1]), [-1, 1])))\n",
    "print(model.config())\n",
    "\n",
    "display(model.summary())\n",
    "\n",
    "save_path = os.path.join(cwd, '..', 'results', 'saved_models', model_name) \n",
    "signatures = {\n",
    "    'resolve': model.resolve,\n",
    "    'config': model.config\n",
    "}\n",
    "tf.keras.models.save_model(model, save_path, include_optimizer=False, save_traces=True, signatures=signatures)\n",
    "\n",
    "[l.output_shape for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'\\n\\x17\\x08\\xb6\\xf7\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x10\\x9c\\x88.\\x1a\\x06\\x08\\x00\\x10\\xbf\\xdc\\x11\\n\\x11\\x08\\xe5\\xca}\\x10\\x8a\\xe9\\x8e\\x01\\x1a\\x06\\x08\\x00\\x10\\xde\\xf6\\x1a\\n\\x12\\x08\\xad\\x8a\\xb3\\x03\\x10\\xc7\\xa4\\xc7\\x03\\x1a\\x06\\x08\\x00\\x10\\x92\\xb7\\x1a\\n\\x12\\x08\\xbb\\xde\\xf1\\x04\\x10\\xfb\\xfb\\x81\\x05\\x1a\\x06\\x08\\x00\\x10\\x83\\xfd\\x19\\n\\x12\\x08\\xc8\\xff\\x9b\\x06\\x10\\xc0\\xce\\xad\\x06\\x1a\\x06\\x08\\x00\\x10\\x94\\xbb\\x1a\\n\\x12\\x08\\x8e\\xba\\xff\\x07\\x10\\xbe\\xad\\x9e\\x08\\x1a\\x06\\x08\\x00\\x10\\x9b\\xb3\\x1a\\n\\x12\\x08\\x8c\\xa7\\x85\\t\\x10\\xac\\x86\\xbe\\t\\x1a\\x06\\x08\\x00\\x10\\xa7\\xd9\\x11']\n",
      " [b\"\\n\\x0f\\x08\\xac'\\x10\\xd6\\xc3\\x06\\x1a\\x06\\x08\\x00\\x10\\xf0\\x97\\x14\\n\\x12\\x08\\xd4\\xec\\xfd\\x02\\x10\\x86\\xd2\\xa4\\x03\\x1a\\x06\\x08\\x00\\x10\\xbf\\x80\\x15\"]\n",
      " [b'\\n\\x17\\x08\\xb6\\xf7\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01\\x10\\x9c\\x88.\\x1a\\x06\\x08\\x00\\x10\\xbf\\xdc\\x11\\n\\x11\\x08\\xe5\\xca}\\x10\\x8a\\xe9\\x8e\\x01\\x1a\\x06\\x08\\x00\\x10\\xde\\xf6\\x1a\\n\\x12\\x08\\xad\\x8a\\xb3\\x03\\x10\\xc7\\xa4\\xc7\\x03\\x1a\\x06\\x08\\x00\\x10\\x92\\xb7\\x1a\\n\\x12\\x08\\xbb\\xde\\xf1\\x04\\x10\\xfb\\xfb\\x81\\x05\\x1a\\x06\\x08\\x00\\x10\\x83\\xfd\\x19\\n\\x12\\x08\\xc8\\xff\\x9b\\x06\\x10\\xc0\\xce\\xad\\x06\\x1a\\x06\\x08\\x00\\x10\\x94\\xbb\\x1a\\n\\x12\\x08\\x8e\\xba\\xff\\x07\\x10\\xbe\\xad\\x9e\\x08\\x1a\\x06\\x08\\x00\\x10\\x9b\\xb3\\x1a\\n\\x12\\x08\\x8c\\xa7\\x85\\t\\x10\\xac\\x86\\xbe\\t\\x1a\\x06\\x08\\x00\\x10\\xa7\\xd9\\x11']\n",
      " [b\"\\n\\x0f\\x08\\xac'\\x10\\xd6\\xc3\\x06\\x1a\\x06\\x08\\x00\\x10\\xf0\\x97\\x14\\n\\x12\\x08\\xd4\\xec\\xfd\\x02\\x10\\x86\\xd2\\xa4\\x03\\x1a\\x06\\x08\\x00\\x10\\xbf\\x80\\x15\"]], shape=(4, 1), dtype=string)\n",
      "sampleRate: 32758\n",
      "\n",
      "Model: \"my_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "fragment_resolver_model_request [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ragged_samples (AudioProcessReq (None, None, 1)      0           fragment_resolver_model_requests[\n",
      "__________________________________________________________________________________________________\n",
      "padded_samples (AudioDataPadder (None, None, 1)      0           ragged_samples[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "splitted_samples (AudioDataUnif ((None, None, None,  0           padded_samples[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "yolo_model (YoloLayer)          (None, None, 20, 5)  1597175     splitted_samples[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "yolo_output_decoder (YoloOutput (None, None, None, 4 0           yolo_model[0][0]                 \n",
      "                                                                 splitted_samples[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "fragment_resolver (FragmentBatc (None, None, 4)      0           yolo_output_decoder[0][0]        \n",
      "                                                                 splitted_samples[0][1]           \n",
      "__________________________________________________________________________________________________\n",
      "audio_proto_encoder (ProtoFragm (None, 1)            0           fragment_resolver[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "resolved_fragments_responses (L (None, 1)            0           audio_proto_encoder[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 1,597,175\n",
      "Trainable params: 1,597,175\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model(\n",
    "    save_path, compile=False, \n",
    "    custom_objects={\n",
    "        'AudioProcessRequestDecoderLayer': AudioProcessRequestDecoderLayer,\n",
    "        'AudioDataPadderLayer': AudioDataPadderLayer,\n",
    "        'AudioDataUniformSplitterLayer': AudioDataUniformSplitterLayer,\n",
    "        'YoloLayer': YoloLayer,\n",
    "        'YoloOutputBatchDecoderLayer': YoloOutputBatchDecoderLayer,\n",
    "        'FragmentBatchResolverLayer': FragmentBatchResolverLayer,\n",
    "        'ProtoFragmentBatchDecoderLayer': ProtoFragmentBatchDecoderLayer,\n",
    "        'LinearTransformerNormalizerLayer': LinearTransformerNormalizerLayer\n",
    "    }\n",
    ")\n",
    "print(new_model(tf.stack([a1, a2, a1, a2])))\n",
    "config = fragment_resolver.FragmentResolverModelConfig()\n",
    "config.ParseFromString(new_model.config()['config'].numpy())\n",
    "print(config)\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca4e8c235f10f497534f8ad40408ba8180408f4f803f54ea5904b45c22e53726"
  },
  "kernelspec": {
   "display_name": "audio-clips-editor",
   "language": "python",
   "name": "audio-clips-editor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
