{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "from model import *\n",
    "import datetime\n",
    "import scipy\n",
    "import librosa\n",
    "import os\n",
    "import generated.fragment_resolver_pb2 as fragment_resolver\n",
    "\n",
    "device = tf.config.experimental.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(samples, num_grid_cells=None, *yolo_outputs):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    color_alpha_per_fragment = 0.5 / len(yolo_outputs)\n",
    "    \n",
    "    encoded_fragments = yolo_decoder(tf.constant(list(yolo_outputs))).merge_dims(0, 1)  \n",
    "    fragment_protos = fragment_decoder.decode_fragments_to_proto(encoded_fragments)[0].numpy()\n",
    "    audio_fragments = fragment_resolver.FragmentResolverModelResponse()\n",
    "    audio_fragments.ParseFromString(fragment_protos)\n",
    "    \n",
    "    for fragment in sorted(audio_fragments.fragments, key=lambda fragment: fragment.startUs):\n",
    "#         fragment_start_offset = int(fragment.startUs / (1e6 / sample_rate))\n",
    "#         fragment_end_offset = int(fragment.endUs / (1e6 / sample_rate))\n",
    "#         fragment_duration = fragment_end_offset - fragment_start_offset\n",
    "        fragment_start_ms = fragment.startUs / 1000\n",
    "        fragment_end_ms = fragment.endUs / 1000\n",
    "        fragment_duration_ms = fragment_end_ms - fragment_start_ms\n",
    "        print(fragment)\n",
    "        rect = plt.Rectangle((fragment_start_ms, -0.5), fragment_duration_ms, 1, fc=(1, 0, 0, color_alpha_per_fragment))\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.plot(np.arange(0, samples.shape[0] / sample_rate * 1000, 1 / sample_rate * 1000), samples, zorder=-1)\n",
    "    if num_grid_cells:\n",
    "        for i_cell in range(num_grid_cells):\n",
    "            rect = plt.Rectangle((i_cell * samples.shape[0] / sample_rate * 1000 / num_grid_cells, -1), samples.shape[0] / sample_rate * 1000 / num_grid_cells, 2, ec='black', fc='none')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "            \n",
    "def plot_stft(samples, fs):\n",
    "    nfft = 1024\n",
    "    nperseg = 1024\n",
    "    noverlap = nperseg-16\n",
    "    samples = np.squeeze(samples, axis=-1)\n",
    "    f, t, stft = scipy.signal.stft(samples, fs, nperseg=nperseg, noverlap=noverlap, nfft=nfft)\n",
    "    print(stft.shape)\n",
    "    threshold_ghz = np.arange(len(f)) < 64\n",
    "    f = f[threshold_ghz]\n",
    "    threshold_ghz = np.transpose(np.tile(threshold_ghz, (t.shape[0], 1)))\n",
    "    stft = np.reshape(stft[threshold_ghz], (f.shape[0], t.shape[0]))\n",
    "    stft = np.abs(stft)\n",
    "    stft = librosa.power_to_db(stft, amin=1e-5, top_db=80.0)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "#     stft = scipy.ndimage.gaussian_filter(stft, sigma=(9, 0))\n",
    "    pcm = ax.pcolormesh(t * 1000, f, stft, shading='gouraud', cmap='inferno')\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "    cbar = fig.colorbar(pcm, ax=ax)\n",
    "    cbar.ax.set_ylabel('Amplitude (dB)')\n",
    "    \n",
    "def is_data():\n",
    "    global data\n",
    "    try:\n",
    "        data = data\n",
    "        return True\n",
    "    except NameError:\n",
    "        return False\n",
    "\n",
    "class EpochCounter(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.epoch_counter = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_counter += 1\n",
    "        print(f'Epoch counter = {self.epoch_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "seed = 4221\n",
    "batch_size = 8\n",
    "sample_rate = 32758\n",
    "min_duration_sec = 5\n",
    "max_shift_duration_sec = 2\n",
    "num_grid_cells = 20\n",
    "# yolo_encoding_type = 'START_DURATION'\n",
    "yolo_encoding_type = 'CENTER_DURATION'\n",
    "fragments_dtype = tf.float32.name\n",
    "\n",
    "input_length = int(sample_rate * min_duration_sec)\n",
    "model_params = {\n",
    "    fragment_resolver.ResolvedTransformer.Type.SILENCE: {\n",
    "        'silenceDurationUs': LinearTransformerNormalizerLayer(5e5, in_dtype=tf.int64)\n",
    "    },\n",
    "#     fragment_resolver.ResolvedTransformer.Type.TYPE2: {\n",
    "#         'typeType2Param2': LinearTransformerNormalizerLayer(100, in_dtype=tf.int64), \n",
    "#           'typeType2Param3': LinearTransformerNormalizerLayer(100, in_dtype=tf.int64), \n",
    "#           'typeType2Param4': LinearTransformerNormalizerLayer(100, in_dtype=tf.int64)\n",
    "#     },\n",
    "#     fragment_resolver.ResolvedTransformer.Type.TYPE3: {\n",
    "#         'typeType3Param5': LinearTransformerNormalizerLayer(100, in_dtype=tf.int64), \n",
    "#         'typeType3Param6': LinearTransformerNormalizerLayer(100, in_dtype=tf.int64)\n",
    "#     }\n",
    "}\n",
    "\n",
    "with open(os.path.join(cwd, 'generated', 'descriptor_set.desc'), 'rb') as desc_file:\n",
    "    protobuf_descriptor = desc_file.read()\n",
    "data_path = os.path.join(cwd, '..', 'data', 'clips', 'labeled')\n",
    "train_data_path = os.path.join(data_path, 'train')\n",
    "val_data_path = os.path.join(data_path, 'train')\n",
    "\n",
    "train_data_generator = AudioDataJsonToProtoGenerator(train_data_path)\n",
    "val_data_generator = AudioDataJsonToProtoGenerator(val_data_path)\n",
    "fragment_encoder = ProtoFragmentBatchEncoderLayer(sample_rate, model_params, protobuf_descriptor, fragments_dtype)\n",
    "fragment_decoder = ProtoFragmentBatchDecoderLayer(sample_rate, model_params, protobuf_descriptor)\n",
    "audio_decoder = AudioDecoder(sample_rate)\n",
    "# Prepare default padding transformer\n",
    "default_padding_transformer = fragment_resolver.ResolvedTransformer() \n",
    "default_padding_transformer.type = fragment_resolver.ResolvedTransformer.Type.SILENCE\n",
    "default_padding_transformer.silenceDurationUs = 0\n",
    "default_padding_transformer = default_padding_transformer.SerializeToString()\n",
    "default_padding_transformer = fragment_encoder.encode_transformers_to_tensor(default_padding_transformer)[0]\n",
    "\n",
    "data_augmenter = AudioDataShiftAugmenterLayer(sample_rate, max_shift_duration_sec, seed)\n",
    "data_padder = AudioDataPadderLayer(sample_rate, min_duration_sec, min_duration_sec * 0.01, default_padding_transformer)\n",
    "data_splitter = AudioDataUniformSplitterLayer(sample_rate, min_duration_sec, min_duration_sec / num_grid_cells * 4, min_duration_sec * 0.01, False)\n",
    "yolo_encoder = YoloOutputEncoderLayer(num_grid_cells, input_length, yolo_encoding_type, False)\n",
    "yolo_decoder = YoloOutputDecoderLayer(input_length, 0.5, yolo_encoding_type, fragments_dtype)\n",
    "yolo_validator = YoloOutputValidatorLayer()\n",
    "\n",
    "# train_source_dataset = tf.data.Dataset\\\n",
    "#             .from_generator(train_data_generator.generate, \n",
    "#                             output_signature=(\n",
    "#                                 tf.TensorSpec(shape=(None), dtype=tf.string),\n",
    "#                                 tf.TensorSpec(shape=(None), dtype=tf.string)\n",
    "#                             ))\n",
    "train_source_dataset = tf.data.experimental.load(os.path.join(data_path, 'train_ds_enc_' + yolo_encoding_type.lower()))\n",
    "# print('Train source dataset loaded')\n",
    "train_dataset = train_source_dataset\\\n",
    "            .cache()\\\n",
    "            .repeat()\\\n",
    "            .map(lambda audio_path, fragments_proto: (audio_path, fragment_encoder.encode_fragments_to_tensor(fragments_proto)))\\\n",
    "            .map(lambda audio_path, fragments: (audio_decoder.decode(audio_path), fragments))\\\n",
    "            .map(data_augmenter.augment_samples)\\\n",
    "            .map(data_padder.pad_samples)\\\n",
    "            .flat_map(data_splitter.split_into_frames)\\\n",
    "            .map(lambda samples, fragments: (samples, yolo_encoder.encode(fragments)))\\\n",
    "            .filter(lambda samples, yolo_output: yolo_validator.is_valid(yolo_output))\\\n",
    "            .shuffle(batch_size * 8, seed, reshuffle_each_iteration=True)\\\n",
    "            .batch(batch_size)\n",
    "\n",
    "# val_source_dataset = tf.data.Dataset\\\n",
    "#             .from_generator(val_data_generator.generate, \n",
    "#                             output_signature=(\n",
    "#                                 tf.TensorSpec(shape=(None), dtype=tf.string),\n",
    "#                                 tf.TensorSpec(shape=(None), dtype=tf.string)\n",
    "#                             ))\\\n",
    "#             .map(lambda audio_path, fragments_proto: (audio_path, fragment_encoder.encode_fragments_to_tensor(fragments_proto)))\\\n",
    "#             .map(lambda audio_path, fragments: (audio_decoder.decode(audio_path), fragments))\\\n",
    "#             .map(data_padder.pad_samples)\\\n",
    "#             .flat_map(data_splitter.split_into_frames)\\\n",
    "#             .map(lambda samples, fragments: (samples, yolo_encoder.encode(fragments)))\\\n",
    "#             .filter(lambda samples, yolo_output: yolo_validator.is_valid(yolo_output))\\\n",
    "#             .cache()\n",
    "val_source_dataset = tf.data.experimental.load(os.path.join(data_path, 'validation_ds_enc_' + yolo_encoding_type.lower()))\n",
    "print('Validation source dataset loaded')\n",
    "val_dataset = val_source_dataset.batch(batch_size)\n",
    "            \n",
    "data = list(val_dataset.take(1).as_numpy_iterator())[0]\n",
    "data = list(zip(data[0], data[1]))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = YoloModel7(input_length, num_grid_cells, fragment_encoder.transformer_output_length)\n",
    "display(model.summary())\n",
    "\n",
    "train_results_filepath = os.path.join(cwd, '..', 'results', 'trained_models')\n",
    "checkpoint_filepath = os.path.join(train_results_filepath, '7_2_cpt.h5')\n",
    "epoch_ctr = EpochCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_bounds_metric',\n",
    "            mode='min',\n",
    "            save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_loss = YoloLossFunction(num_grid_cells, input_length, \n",
    "                {'l_cbf': 35.0, 'l_cubf': 30.0, 'l_isobj': 1.0, 'l_noobj': 0.5, 'l_class': 35.0}, yolo_encoding_type)\n",
    "\n",
    "start_metric = YoloLossFunction(num_grid_cells, input_length,\n",
    "                {'l_cbf': 1.0, 'l_cubf': 0.0, 'l_isobj': 0, 'l_noobj': 0, 'l_class': 0}, yolo_encoding_type, name='start_metric')\n",
    "duration_metric = YoloLossFunction(num_grid_cells, input_length,\n",
    "                {'l_cbf': 0.0, 'l_cubf': 1.0, 'l_isobj': 0, 'l_noobj': 0, 'l_class': 0}, yolo_encoding_type, name='duration_metric')\n",
    "class_metric = YoloLossFunction(num_grid_cells, input_length,\n",
    "                {'l_cbf': 0, 'l_cubf': 0, 'l_isobj': 0, 'l_noobj': 0, 'l_class': 1.0}, yolo_encoding_type, name='class_metric')\n",
    "bounds_metric = YoloLossFunction(num_grid_cells, input_length,\n",
    "                {'l_cbf': 1.0, 'l_cubf': 1.0, 'l_isobj': 0, 'l_noobj': 0, 'l_class': 0}, yolo_encoding_type, name='bounds_metric')\n",
    "\n",
    "model.compile(\n",
    "    loss=yolo_loss,\n",
    "    metrics=[start_metric, duration_metric, class_metric, bounds_metric],\n",
    "#     optimizer='adagrad'\n",
    "#     optimizer='rmsprop'\n",
    "#     optimizer='adam'\n",
    "#     optimizer=tf.keras.optimizers.SGD(learning_rate=0.0001)#, clipnorm=0.01)\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "#     optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001, centered=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "while epoch_ctr.epoch_counter < num_epochs:\n",
    "    try:\n",
    "        model.fit(train_dataset, epochs=num_epochs, initial_epoch=epoch_ctr.epoch_counter, \n",
    "                  verbose=1, steps_per_epoch=640/batch_size,\n",
    "                  validation_data=val_dataset, \n",
    "                  callbacks=[epoch_ctr, model_checkpoint_callback])\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if is_data():\n",
    "    data_indexes = [i for i in range(0, 4)]\n",
    "    for data_index in data_indexes:\n",
    "        predicted = model.predict(tf.constant([data[data_index][0]]))[0]\n",
    "        plot_data(data[data_index][0], num_grid_cells, predicted, data[data_index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate.assign(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(train_results_filepath, '7_2_cpt.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(os.path.join(train_results_filepath, '7_1.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_loss.set_lambdas({\n",
    "    'l_start': 10.0, 'l_duration': 10.0, 'l_isobj': 1.0, \n",
    "    'l_noobj': 0.5, 'l_class': 10.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(train_source_dataset, os.path.join(data_path, 'train_ds_enc_' + yolo_encoding_type.lower()))\n",
    "print('saved train dataset')\n",
    "tf.data.experimental.save(val_source_dataset, os.path.join(data_path, 'validation_ds_enc_' + yolo_encoding_type.lower()))\n",
    "print('saved validation dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if is_data():\n",
    "    data_indexes = [i for i in range(0, 4)]\n",
    "    for data_index in data_indexes:\n",
    "        plot_data(data[data_index][0], 0, data[data_index][1])\n",
    "        plot_stft(data[data_index][0], sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import shutil\n",
    "    \n",
    "fraction = 5\n",
    "for i, src_datapath in enumerate(filter(os.path.isfile, map(lambda filename: os.path.join(data_path, filename), os.listdir(datapath)))):\n",
    "    if i % fraction == 0:\n",
    "        target_datapath = os.path.join(data_path, 'validation')\n",
    "    else:\n",
    "        target_datapath = os.path.join(data_path, 'train')\n",
    "    a = shutil.copy2(src_datapath, target_datapath)\n",
    "    print('copied', a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-clips-editor",
   "language": "python",
   "name": "audio-clips-editor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
